---
layout: page
title: Chapter 8 - PAC learning primer and error bounds
categories: generalization
date: 2022-01-11
---

OK, so implicit regularization (via the choice of training algorithm) may be part of the reason for generalization, but almost surely not the entire picture.

It may be useful to revisit classical ML here. Very broadly, the conventional thinking has been:

```
Parsimony enables generalization.
```

The challenge is to precisely define what "parsimony" means here, since there are a myriad different ways of doing this. The bulk of work in generalization theory explores more and more refined complexity measures of ML models, but as we will see below, most existing classical approaches lead to *vacuous* bounds for deep networks. Getting non-vacuous generalization guarantees is a major challenge.

## Setup
{:.label}

## Complexity measures
{:.label}

### Agnostic (PAC) learning

### Data-dependent bounds

## Error bounds for deep networks
{:.label}

## PAC-Bayes
{:.label}
